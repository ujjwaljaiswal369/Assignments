{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d8e9d5-890d-45b9-9940-8d29dca0b73a",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f521a0-e4f7-48ea-949d-6b0e0e49b8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be \n",
    "       mitigated?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d185d-14a8-4d7e-bad2-5db07211c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans. \n",
    "\n",
    "# Brief Definition\n",
    "Overfitting and underfitting in machine learning is condition of a model, The overfitting and underfitting is determined\n",
    "by the analyzing the training accuracy and test accuracy. Whenever we evaluate the performance of the models through \n",
    "metrices then we know about the training accuracy and test accuracy \n",
    "\n",
    "Overfitting = Whenever the training accuracy of the model is high and the test accuracy is low then we can say this type\n",
    "              of condition is called Overfitting. We should also know about the bias and variance. In case of overfitting\n",
    "              if training accuracy is high and test accuracy is low then it is low bias and high variance.\n",
    "        \n",
    "              In case of overfitting, the best fit line is passing through the every training datapoints and less passes\n",
    "              through test datapoints thats why overfitting is occur.\n",
    "        \n",
    "Underfitting = Whenever the training accuracy is low and test accuracy is also low then in this case we can say that it is\n",
    "               underfitting model. And in this type of condition training accuracy and test accuracy is low then it is \n",
    "               high bias and high variance.\n",
    "               In case of underfitting, The best fit line is not properly passes through both training datapoints and test\n",
    "               datapoints. The regression line is under fitted means not properly fitted. So it is called underfitting \n",
    "        \n",
    "\n",
    "# Mitigated        \n",
    "We can mitigated the overfitting and underfitting by some techniques. the underfitting and overfitting is also determined\n",
    "by training error and test error. If the training error is high then training accuracy is low and if high then accuracy is\n",
    "also high. Same case is happen in test error also, So for reducing the overfitting and underfitting we should focus on \n",
    "handling on training error and test error.\n",
    "For reducing overfitting, we can use ridge and lasso technique, Reducing model complexity, and increasing the training data\n",
    "For reding underfitting, we can perform some technique like handling the bias and variance , removing the noise from data\n",
    "\n",
    "\n",
    "# Consequences\n",
    "\n",
    "If the overfitting and underfitting conditions is occured in model then it is not good for model because it will predict\n",
    "poor accuracy in both cases. the underfit predict the high training error and high test error also. And in case of overfit\n",
    "the prediction are very low of training error and test error is very high. For prevent from this error we should focus on\n",
    "model improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1ec59-b0be-4b4d-9fc7-54052d321683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6ff5e4-750a-4a48-ba5d-047487e5f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q2: How can we reduce overfitting? Explain in brief.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394b724c-595a-4b3e-9913-ba2f188f2aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans.\n",
    "Overfitting in machine learning is a phase which can identify in models. When the training accuracy is so high and the \n",
    "test accuracy is very low then in this case we can say it is overfitting. And it is due to low bias and high variance.\n",
    "We can identify the training accuracy and test accuracy by using performances matrices and after we can observe either \n",
    "model is suffered from overfitting or underfitting. \n",
    "In case of overfitting, the best fit line or regression line is completly passes through training datapoints and in that \n",
    "case training error is low and test accuracy is very low because best fit line is not passes through test datapoints.\n",
    "When the model have overfitting case then we have to take action for reducing overfitting, for reducing overfitting we \n",
    "have some techniques to increase test accuracy and decrese the variance. \n",
    "We can use some technique for reducing overfitting like,\n",
    "\n",
    "1. Increase training dataset:\n",
    "    For reducing overfitting we can use data augmentation or in another words increasing the training dataset. The \n",
    "    increase training dataset can reduce overfitting in model. When we are constrained with dataset we have then we can\n",
    "    applying the data augmentation technique which increase dataset size artificially. \n",
    "    How Inceasing dataset helps : When data augmentation is applying then it make training dataset more unique to model\n",
    "    and prevents the model from learning their characteristics. For example, applying Transformations like translation,\n",
    "    flipping, rotation to input images,etc\n",
    "\n",
    "    \n",
    "2. Using Regularization:\n",
    "    The Regularization is a collection of optimisation techniques for reducing the overfitting. We can use ridge and \n",
    "    lasso for regularization. These methods try to eliminate those factors which have not greater impact on predictions \n",
    "    outcomes by grading feature.\n",
    "    For example, Using the mathmatical calculations can apply penalty value to feature with minimal impact.\n",
    "    \n",
    "    \n",
    "3. Decreasing test error accuracy:\n",
    "    The cause of overfitting is increasing the test error, then we should try to decrease the test error by handling \n",
    "    test error by minimize the variance.\n",
    "    \n",
    "    \n",
    "4. Using ensemble techniques :\n",
    "    Ensemble technique is a technique which combines the prediction of machine learning algoritms. Some models are weak \n",
    "    because they predict the inaccurate predictions then for preventing this problem we can combine more than one model \n",
    "    which give the accurate prediction than before prediction. There are mainly two type of ensemble techniques; Bagging\n",
    "    and Boosting. Bagging is train the machine learning model one after one to get final result and Boosting train the\n",
    "    model parallelly.\n",
    "    \n",
    "    \n",
    "5. Prunning the Features and parameters :\n",
    "    Prunning the features and parameters in model can help in reducing overfitting. The feature selection or selecting the\n",
    "    important parameter is identify the important features and ignore the irrelevant. \n",
    "    Suppose you make model of predicting the image of animal and human then you train the model like identify the nose,\n",
    "    hair, shape, ear,etc which is important parameter for prediction but you ignore shape of eyes because it is clearly\n",
    "    irrelevant for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf3de5-1441-4e40-beed-18a081e8341f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd2f26-44a2-4bd4-8c88-bd4ec48c106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q3: Explain underfitting. List scenarios where underfitting can occur in ML.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff14c8f-269c-4594-861a-42202607996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans.\n",
    "\n",
    "#Definition\n",
    "Underfitting is also a phase or problem like overfitting that occur in machine learning models. We can calculate the \n",
    "training accuracy and test accuracy by performance matrix When the training accuracy of the model are low as well as the\n",
    "test accuracy is also low then we can say it is underfitting. \n",
    "\n",
    "#Occurance\n",
    "In case of Underfitting, The regression line is fitted under the datapoints or in another words when the regression line \n",
    "is not paases through both training datapoints and test datapoints then underfitting problem is occur. And the reason for\n",
    "underfitting is due to high bias and high variance. The one more reason behind the occurance of underfitting is due to \n",
    "high training error and high test error. However, The uniform features are interpret the model accuracy because due to\n",
    "similarities model cannot understand domiant trend properly which leads the underfitting.\n",
    "\n",
    "#reducting underfitting\n",
    "Like overfitting, we can also reduce underfitting in models through some technique, for preventing the underfitting we \n",
    "should first keep eye on training error and test error, We should try to minimize the training error and test error.\n",
    "Apart from above, we can apply below techniques also;\n",
    "\n",
    "\n",
    "1. Regularization Technique = In regularization, it eliminate the unused factor which are not relevant. By using lasso\n",
    "                              regularisation, L1 regularisation,etc we can remove outliers and noise from the dataset \n",
    "                              which prevent from reducing the underfitting. \n",
    "    \n",
    "    \n",
    "2. Increasing Training Duration = The stopping too earlier training is leads to underfitting. Therefore, we should \n",
    "                                  increase the duration of training. As we know , the consequences of overtraining. but \n",
    "                                  if we balance training duration between these two scenerio then it will good for model.\n",
    "    \n",
    "\n",
    "3. Increasing features = The specific features predict the derisable outcomes of model. So, For reducing underfitting we \n",
    "                         can also handle it by increasing the no. of features by feature engineering and feature selection\n",
    "    \n",
    "4.  Model complexity = Sometimes, increasing of model complexity can also helps in case of preventing underfitting. Model\n",
    "                       complexity is determined the nature of training data. Then for preventing underfitting we have to \n",
    "                       work on training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9f546-bd0c-4012-8c99-79def9be5094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf22388f-1acb-4725-8eb3-7ab1a70374d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and \n",
    "       how do they affect model performance?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9cd53c-87d7-4d0f-8603-04ed7c994575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans. \n",
    "Bias and variance are a prediction errors which tell the condition of model in machine learning. Bias and variance are \n",
    "correlated with training and test accuracy. \n",
    "The difference of prediction between the actual and predicted datapoints of machine learning model is called bias. And \n",
    "the variability of model pridiction which shows the spreadness of data is called variance. The variance is inversly \n",
    "related with test accuracy, when the test accuracy is high then the variance is low and when the test accuracy is high \n",
    "then variance is low. \n",
    "\n",
    "Same happen with bias and training accuracy also, when training accuracy is high then bias is low and when training \n",
    "accuracy is low then bias is high. The something relation betweem Bias and variance is known as bias and variance tradeoff\n",
    "Bias and Variance tradeoff is a vice versa relation between bias and variance. If the model is simple and having a few\n",
    "parameters then it have high bias and low variance and if the no. of parameters are large then the low bias and high \n",
    "variance incident is occur. So for good model we have to find a balance between them , this is known as bias-variance \n",
    "tradeoff.\n",
    "\n",
    "\n",
    "The bias and variance is also related with the underfitting and overfitting. when the bias is low and variance is high \n",
    "then this case will considered as overfitting and if bias is high and variance is high then this is underfitting. The \n",
    "machine learning model needs a generalised model which regression line is equally close from test datapoints and training\n",
    "datapoints. \n",
    "\n",
    "The bias and variance is affect the model by their high and low. The overfitting and underfitting cases is generated due\n",
    "to bias and variance as we know already that the low bias and high variance cause overfitting and high bias and high \n",
    "variance can cause underfitting. The overfitting means over fit regression line to training datapoints and underfitting\n",
    "is just opposite of overfitting regression line is far away from training datapoints. \n",
    "When the overfitting and underfitting is occur in model then it very affect machine learning model. If it both are occur\n",
    "in particular model then it means that model will suffered from lots of outlier , lots of noise, improper data, repeated\n",
    "features or similar data which interpret the domiant trend of dataset. So we should look for a generalised model which \n",
    "have low bias and low variance and give desirable predictions. We have to work on bias and variance and try to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35946145-cac7-4401-8d86-6b36de03cba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e83e5-7cc6-4581-9c17-31c33d0b56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you \n",
    "       determine whether your model is overfitting or underfitting?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eed85b-ed11-4e09-9058-bb81a6c7ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans.\n",
    "We can detect the overfitting and underfitting simply by just analyzing the bias and variance because bias and variance \n",
    "have great role in detecting the overfitting and underfitting. Methods for detecting the overfitting and underfitting:\n",
    "    \n",
    "    1. When the bias is high and the variance is high then we can considered it as a underfitting\n",
    "    2. And if the bias is low and the variance is high then we can considered it as overfitting. \n",
    "    \n",
    "The bias and variance are prediction errors, The difference between the actual point and prediction point then this is \n",
    "called as bias and variance is a variablity of the model which shows the spreadness of data that how the data is diverting\n",
    "from mean position in grpah. \n",
    "We can determine either our machine learning model have overfitting or underfitting problem by training error and test \n",
    "error. The highness of training error and test error are tells the model needs cleaning and we can find training error \n",
    "and test error by training dataset and test dataset through regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ef50b-3449-4d3e-8d8a-d4d2a70b071e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede6a9f-db0c-463f-a187-d07cb6476b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance\n",
    "       models, and how do they differ in terms of their performance?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6082d2-580b-4f1a-bea3-9b9b1f0c9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans.\n",
    "Bias and Variance in machine learning are a predictions error. Bias in machine learning refers to a difference between \n",
    "prediction point and actual point and Variance in machine learning is variability of model which shows the spreadness of \n",
    "data and tell how much data is deviate from the mean. \n",
    "Due to high and low of bias and variance, the overfitting and underfitting problem is occur in machine learning model. \n",
    "When the bias is low and variance is high then overfitting problem is occured in machine learning model and when bias is\n",
    "low and variance is also low then underfitting problem is occured in machine learning model.\n",
    "\n",
    "High bias moedels : Some examples of high bias models are Linear regression,Logistics regression, Discriminant analysis,\n",
    "                    etc. the bias models are less friendly to training dataset. The high bias model have noise and outliers\n",
    "                    which interpret in model performance \n",
    "    \n",
    "High variance models : Some examples of high models are Decision tree, K nearest neighbour, Support Vector machines.\n",
    "                       High variance models are not able to predict the data accuratly. which also affect the accuracy and\n",
    "                       performance of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e1af1-87da-4d76-89e8-a1f9a0d09295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2c3c0-926d-4e50-ba34-d2130df55a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common \n",
    "       regularization techniques and how they work.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78587f17-398b-45e3-b908-bfbd671359ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans.\n",
    "\n",
    "The regularization in machine learning is a collection of training or optimisers techniques which helps in reducing \n",
    "overfitting. Overfitting is happen due to low bias and high variance and due to training and test error for preventing\n",
    "this error the regularization is used which eliminate the outlier and noise in dataset and eliminate the irrelevant \n",
    "factors which are not play role in predicting the outcomes.This technique are helps in reducing the overfitting by \n",
    "fitting a appropriately function to a training dataset.\n",
    "\n",
    "For example, Using the mathmatical calculations can apply penalty value to feature with minimal impact.\n",
    "\n",
    "There are many techniques of regularization :\n",
    "    \n",
    "    1. Lasso regularization\n",
    "    2. Ridge regularization\n",
    "    3. Elastic net regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc17aa2-b8ff-4af5-a940-1b0333f667ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Lasso regularization : \n",
    "    A regression model which uses the L1 regularisation technique then it is known as Lasso regression(Least Absolute\n",
    "    Shrinkage and Selection Operator). Lasso regularization add the 'absolute value of magnitude' of the coefficient as a\n",
    "    penalty term to a loss function(L)."
   ]
  },
  {
   "attachments": {
    "ac3ba54d-87ea-4048-8cf8-8642f16e5254.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAL0AAAAhCAYAAACMc6oIAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nTa6xFZORVQAAAABJRU5ErkJggg==\n"
    }
   },
   "cell_type": "markdown",
   "id": "27dceb87-506c-4f55-b6d5-06acb3b958a1",
   "metadata": {},
   "source": [
    "![Screenshot_1.png](attachment:ac3ba54d-87ea-4048-8cf8-8642f16e5254.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255811a7-032a-4d52-a6aa-a07cabe3b7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Ridge regularisation:\n",
    "    A regression model which uses the L2 regularisation technique then it is known as Ridge regularization. The ridge \n",
    "    regularization adds the 'squared magnitude' of a coefficient as a penalty term to a loss function(L)."
   ]
  },
  {
   "attachments": {
    "2256acf5-2657-4692-ade1-81ceed5f42d4.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAALYAAAAiCAYAAADyByNRAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nTtiOWOKE7YglTtiOGCLyP9B3Fjixr9F+AAAAAElFTkSuQmCC\n"
    }
   },
   "cell_type": "markdown",
   "id": "e199da15-e78d-4276-a845-b53f2870ee3d",
   "metadata": {},
   "source": [
    "![Screenshot_2.png](attachment:2256acf5-2657-4692-ade1-81ceed5f42d4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453f343-6896-45a1-abee-bf9e4b261866",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Elastic net regularisation : \n",
    "    This model is combination of L1 and L2 regularisation. This impies that we add the absolute norm of weight as well as\n",
    "    squared measure of weight. With the help of extra hyperparameter that control the ratio of L1 and L2 regularization."
   ]
  },
  {
   "attachments": {
    "36b9aede-81ef-45ca-aeac-4af7f81275e4.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAASkAAAAlCAYAAAAeLB9NAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\n"
    }
   },
   "cell_type": "markdown",
   "id": "5f2588ee-928f-4343-9a2e-51132ab79f2f",
   "metadata": {},
   "source": [
    "![Screenshot_3.png](attachment:36b9aede-81ef-45ca-aeac-4af7f81275e4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f587e32-6e2e-4b89-8859-1a714507ebdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
